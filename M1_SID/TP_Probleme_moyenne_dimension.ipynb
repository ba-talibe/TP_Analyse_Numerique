{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from CheckMediumScale_init import create_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude du probleme Ã  moyenne dimension     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude preliminaire  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude numerique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generation of A,b et c\n",
    "\n",
    "a, b, c = create_problem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "def golden_search(f, a, b, epsilon=10**-4):\n",
    "    phi = (1 + np.sqrt(5)) / 2\n",
    "    while b - a > epsilon:\n",
    "        c = a + (b - a) / (phi + 1)\n",
    "        d = b - (b - a) / (phi + 1)\n",
    "        if f(c) < f(d):\n",
    "            b = d\n",
    "        else:\n",
    "            a = c\n",
    "    return (a + b) / 2\n",
    "\n",
    "\n",
    "def backtracking(cost, gradient, x, sigma_init=2, c=0.1, roo=0.5):\n",
    "    dk = -gradient(x)\n",
    "    while cost(x + sigma_init*dk) >  - c*sigma_init*np.linalg.norm(dk)**2:\n",
    "        sigma_init = roo*sigma_init\n",
    "    return sigma_init\n",
    "\n",
    "def get_step_function(cost, gradient, x, name='golden_search'):\n",
    "    if name == 'golden_search':\n",
    "        return golden_search(lambda sigma : cost(x - sigma*gradient(x)), 0, 2)\n",
    "    elif name == 'backtracking':\n",
    "        return backtracking(cost, gradient, x)\n",
    "    else:\n",
    "        return 1e-4\n",
    "    \n",
    "def steepest_descent(x0,cost,gradient, step_f_name=\"golden_search\", epsilon=1e-2, maxiter=100000):\n",
    "    xlist = [x0] # list of points\n",
    "    flist = [cost(x0)] # list of cost function  values\n",
    "    nlist = [np.linalg.norm(gradient(x0))] # list of gradient norm values\n",
    "    it = 0\n",
    "\n",
    "    x = xlist[-1]\n",
    "    grad = gradient(x)\n",
    "    step = get_step_function(cost, gradient, x, name=step_f_name)\n",
    "    xk = x - step*grad \n",
    "    xlist.append(xk)\n",
    "    flist.append(cost(xk))\n",
    "    nlist.append(np.linalg.norm(gradient(xk)))\n",
    "    while np.abs(flist[-2] - flist[-1]) > epsilon:\n",
    "        x = xlist[-1]\n",
    "        grad = gradient(x)\n",
    "        step = get_step_function(cost, gradient, x, name=step_f_name)\n",
    "        xk = x - step*grad \n",
    "        xlist.append(xk)\n",
    "        flist.append(cost(xk))\n",
    "        nlist.append(np.linalg.norm(gradient(xk)))\n",
    "        it += 1\n",
    "        if it > maxiter:\n",
    "            break\n",
    "    return xlist,flist,nlist\n",
    "\n",
    "\n",
    "def visualize_gradient_descent(cost, gradient, xstar, step=\"golden_search\", epsilon=1e-4, maxiter=100000):\n",
    "    fmin  = cost(xstar)\n",
    "\n",
    "    #grid\n",
    "    ax= xstar[0]\n",
    "    bx = xstar[0]\n",
    "    ay= xstar[1]\n",
    "    by = xstar[1]\n",
    "\n",
    "    for test in range(2):\n",
    "        x0 = np.copy(xstar) + 2.*(np.random.rand(2)-.5)\n",
    "        #\n",
    "        fig = plt.figure(1, figsize=(19,5))\n",
    "        xlist,flist,nlist =  steepest_descent(x0,cost,gradient,step_f_name=\"golden_search\")\n",
    "        xlist = np.asarray(xlist)\n",
    "        #\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.plot(xlist[:,0], xlist[:,1],'o-',label='points')\n",
    "        #    #\n",
    "        ax = np.min((xlist[:,0].min(),ax))-.1\n",
    "        bx = np.max((xlist[:,0].max(),bx))+.1\n",
    "        ay = np.min((xlist[:,1].min(),ay))-.1\n",
    "        by = np.max((xlist[:,1].max(),by))+.1\n",
    "        \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.semilogy(range(len(flist)),flist-fmin+1e-16)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel(r'$f(x^k)$')\n",
    "        \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.semilogy(nlist,':')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel(r'$f(x^k)$')\n",
    "        \n",
    "        \n",
    "    #    \n",
    "    xgrid = np.arange(ax,bx,(bx-ax)/50)\n",
    "    ygrid = np.arange(ay,by,(by-ay)/50)\n",
    "    X, Y = np.meshgrid(xgrid, ygrid)\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            Z[i,j] = cost(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.contour(X, Y, Z,21)\n",
    "\n",
    "    plt.plot(xstar[0], xstar[1],'*',label='points')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "step_funtion =  [\"golden_search\", \"backtracking\", \"constant_step\"]\n",
    "\n",
    "# for step_f in step_funtion:\n",
    "#     print(\"Step function: \", step_f)\n",
    "#     visualize_gradient_descent(cost, gradient, xstar, step=step_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
